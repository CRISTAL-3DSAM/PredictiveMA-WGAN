{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0873e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874f76bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n",
      "WARNING:root:Use of the keyword argument names (flag_name, default_value, docstring) is deprecated, please use (name, default, help) instead.\n"
     ]
    }
   ],
   "source": [
    "Training = False\n",
    "Short_Term=False\n",
    "flags = tf.compat.v1.flags\n",
    "flags.DEFINE_integer(flag_name='epoch', default_value=500, docstring='number of epochs')\n",
    "flags.DEFINE_integer(flag_name='nb_joints', default_value=17, docstring='number of joint in the skeletons')\n",
    "flags.DEFINE_integer(flag_name='Coeff_Gram', default_value=10, docstring='coeff of gram loss')\n",
    "flags.DEFINE_integer(flag_name='Coeff_bone', default_value=10, docstring='coeff of bone loss')\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "if Short_Term:\n",
    "    flags.DEFINE_integer(flag_name='number_frames', default_value=11, docstring='number of frames to generate')\n",
    "    flags.DEFINE_integer(flag_name='number_frames_prior', default_value=10, docstring='number of frames to use as prior')\n",
    "    flags.DEFINE_string(flag_name='data_dir', default_value='Data_skeleton', docstring='name the directory for the data')  # can be different for long term\n",
    "    flags.DEFINE_string(flag_name='load_train', default_value='Data_skeleton/Data_train_short.txt', docstring='train file to load')\n",
    "    flags.DEFINE_string(flag_name='load_test', default_value='Data_skeleton/Data_test_short.txt', docstring='test file to load')\n",
    "    flags.DEFINE_string(flag_name='load_qmean', default_value='Data_skeleton/q_mean_data_short.mat', docstring='qmean to load')\n",
    "    flags.DEFINE_string(flag_name='save_dir', default_value='save_short', docstring='dir for saving training results')\n",
    "    flags.DEFINE_string(flag_name='checkpoint_dir', default_value='Checkpoint_short', docstring='dir for loading checkpoints')\n",
    "    flags.DEFINE_string(flag_name='checkpoint_name', default_value='model-135000', docstring='name of checkpoint file, model-XXXXX')\n",
    "    flags.DEFINE_string(flag_name='generated_dir', default_value='generated_samples_short', docstring='name the directory for generated samples')\n",
    "    if Training:\n",
    "        flags.DEFINE_boolean(flag_name='is_train', default_value=True, docstring='training mode')\n",
    "        flags.DEFINE_integer(flag_name='batch_size', default_value=64, docstring='number of batch size')\n",
    "\n",
    "    else:\n",
    "        flags.DEFINE_boolean(flag_name='is_train', default_value=False, docstring='training mode')\n",
    "        flags.DEFINE_integer(flag_name='batch_size', default_value=812, docstring='number of samples in test set')\n",
    "\n",
    "else:\n",
    "    flags.DEFINE_integer(flag_name='number_frames', default_value=26, docstring='number of frames to generate')\n",
    "    flags.DEFINE_integer(flag_name='number_frames_prior', default_value=25, docstring='number of frames to use as prior')\n",
    "    flags.DEFINE_string(flag_name='data_dir', default_value='Data_skeleton', docstring='name the directory for the data')\n",
    "    flags.DEFINE_string(flag_name='load_train', default_value='Data_skeleton/Data_train_long.txt', docstring='train file to load')\n",
    "    flags.DEFINE_string(flag_name='load_test', default_value='Data_skeleton/Data_test_long.txt', docstring='test file to load')\n",
    "    flags.DEFINE_string(flag_name='load_qmean', default_value='Data_skeleton/q_mean_data_long.mat', docstring='qmean to load')\n",
    "    flags.DEFINE_string(flag_name='save_dir', default_value='save_long', docstring='dir for saving training results')\n",
    "    flags.DEFINE_string(flag_name='checkpoint_dir', default_value='Checkpoint_long', docstring='dir for loading checkpoints')\n",
    "    flags.DEFINE_string(flag_name='checkpoint_name', default_value='model-107500', docstring='name of checkpoint file, model-XXXXX')\n",
    "    flags.DEFINE_string(flag_name='generated_dir', default_value='generated_samples_long', docstring='name the directory for generated samples')\n",
    "    if Training:\n",
    "        flags.DEFINE_boolean(flag_name='is_train', default_value=True, docstring='training mode')\n",
    "        flags.DEFINE_integer(flag_name='batch_size', default_value=64, docstring='number of batch size')\n",
    "\n",
    "    else:\n",
    "        flags.DEFINE_boolean(flag_name='is_train', default_value=False, docstring='training mode')\n",
    "        flags.DEFINE_integer(flag_name='batch_size', default_value=644, docstring='number of samples in test set')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d3b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_conv2d(input_map, num_output_channels, size_kernel=3, stride=1, stddev=0.02, name=\"conv2d\"):\n",
    "        ## output size is in format: NHWC\n",
    "        with tf.compat.v1.variable_scope(name):\n",
    "            kernel = tf.compat.v1.get_variable(\n",
    "                name='w',\n",
    "                shape=[size_kernel, size_kernel, input_map.get_shape()[-1], num_output_channels],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.compat.v1.truncated_normal_initializer(stddev=stddev)\n",
    "            )\n",
    "            conv = tf.nn.conv2d(input_map, kernel, strides=[1, stride, stride, 1], padding='SAME')\n",
    "            return conv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de9726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input_map, num_output_channels, size_kernel=5, stride=2, name='conv2d'):\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        stddev = 0.02 ### np.sqrt(2.0 / (np.sqrt(input_map.get_shape()[-1].value * num_output_channels) * size_kernel ** 2))\n",
    "        kernel = tf.compat.v1.get_variable(\n",
    "            name='w',\n",
    "            shape=[size_kernel, size_kernel, input_map.get_shape()[-1], num_output_channels],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.compat.v1.truncated_normal_initializer(stddev=stddev)\n",
    "        )\n",
    "        print (kernel.get_shape())\n",
    "        biases = tf.compat.v1.get_variable(\n",
    "            name='b',\n",
    "            shape=[num_output_channels],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.constant_initializer(0.0)\n",
    "        )\n",
    "        conv = tf.nn.conv2d(input_map, kernel, strides=[1, stride, stride, 1], padding='SAME')\n",
    "        return tf.nn.bias_add(conv, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19c3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(input_vector, num_output_length, name='fc'):\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        stddev = np.sqrt(1.0 / (np.sqrt(input_vector.get_shape()[-1] * num_output_length)))\n",
    "        w = tf.compat.v1.get_variable(\n",
    "            name='w',\n",
    "            shape=[input_vector.get_shape()[1], num_output_length],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.random_normal_initializer(stddev=stddev)\n",
    "        )\n",
    "        b = tf.compat.v1.get_variable(\n",
    "            name='b',\n",
    "            shape=[num_output_length],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.constant_initializer(0.0)\n",
    "        )\n",
    "        return tf.matmul(input_vector, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "193c9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv2d(input_map, output_shape, size_kernel=5, stride=2, stddev=0.02, name='deconv2d'):\n",
    "    with tf.variable_scope(name):\n",
    "        stddev = np.sqrt(1.0 / (np.sqrt(input_map.get_shape()[-1].value * output_shape[-1]) * size_kernel ** 2))\n",
    "        kernel = tf.get_variable(\n",
    "            name='w',\n",
    "            shape=[size_kernel, size_kernel, output_shape[-1], input_map.get_shape()[-1]],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.random_normal_initializer(stddev=stddev)\n",
    "        )\n",
    "        biases = tf.get_variable(\n",
    "            name='b',\n",
    "            shape=[output_shape[-1]],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.constant_initializer(0.0)\n",
    "        )\n",
    "        deconv = tf.nn.conv2d_transpose(input_map, kernel, strides=[1, stride, stride, 1], output_shape=output_shape)\n",
    "        return tf.nn.bias_add(deconv, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975ce407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(logits, leak=0.2):\n",
    "    return tf.maximum(logits, leak*logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e53a8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_label(x, label, duplicate=1):\n",
    "    x_shape = x.get_shape().as_list()\n",
    "    if duplicate < 1:\n",
    "        return x\n",
    "    label = tf.tile(label, [1, duplicate])\n",
    "    label_shape = label.get_shape().as_list()\n",
    "    if len(x_shape) == 2:\n",
    "        return tf.concat(1, [x, label])\n",
    "    elif len(x_shape) == 4:\n",
    "        label = tf.reshape(label, [x_shape[0], 1, 1, label_shape[-1]])\n",
    "        return tf.concat(3, [x, label*tf.ones([x_shape[0], x_shape[1], x_shape[2], label_shape[-1]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d0b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_label_newtf(x, label, duplicate=1):\n",
    "    x_shape = x.get_shape().as_list()\n",
    "    if duplicate < 1:\n",
    "        return x\n",
    "    label = tf.tile(label, [1, duplicate])\n",
    "    label_shape = label.get_shape().as_list()\n",
    "    if len(x_shape) == 2:\n",
    "        return tf.concat([x, label],1)\n",
    "    elif len(x_shape) == 4:\n",
    "        label = tf.reshape(label, [x_shape[0], 1, 1, label_shape[-1]])\n",
    "        return tf.concat( [x, label*tf.ones([x_shape[0], x_shape[1], x_shape[2], label_shape[-1]])],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f553bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(object):\n",
    "    def __init__(self,\n",
    "                 session,\n",
    "                 size_SRVF_H=51,\n",
    "                 size_SRVF_W=26,\n",
    "                 nb_frames=10,\n",
    "                 Gram_matrix_size = 25,\n",
    "                 size_kernel=5,\n",
    "                 #size_batch=128,\n",
    "                 size_batch=3, # taille test\n",
    "                 num_encoder_channels=64,\n",
    "                 num_z_channels=50,\n",
    "                 num_input_channels=1,\n",
    "                 y_dim=2, # pas utilise\n",
    "                 rb_dim=3, # pas utilise\n",
    "                 test_batch_low=0,\n",
    "                 test_batch_up=0,\n",
    "                 num_gen_channels=512,\n",
    "                 enable_tile_label=True,\n",
    "                 tile_ratio=1.0,\n",
    "                 is_training=True,\n",
    "                 disc_iters=4,  # For WGAN and WGAN-GP, number of descri iters per gener iter\n",
    "                 is_flip=True,\n",
    "                 data_dir = 'data_skeleton',\n",
    "                 load_train = 'Data_skeleton/Data_train.txt',\n",
    "                 load_test = 'Data_skeleton/Data_test.txt',\n",
    "                 load_qmean = 'Data_skeleton/q_mean_data.mat',\n",
    "                 discription='HUMAN',\n",
    "                 checkpoint_dir='./checkpoint',\n",
    "                 checkpoint_name = 'model-135000',\n",
    "                 save_dir='Results/',\n",
    "                 generated_dir='genrated_samples_long',\n",
    "                 num_epochs=200,\n",
    "                 learning_rate=0.0001,\n",
    "                 LAMBDA=10,  # Gradient penalty lambda hyperparameter\n",
    "                 coeff_skel_loss = 100,# coefficient for loss on skeleton position\n",
    "                 Skel_links = np.array([[1,2],[1,5],[1,8],[2,3],[3,4],[5,6],[6,7],[8,9],[9,10],[10,11],[9,12],[12,13],[13,14],[9,15],[15,16],[16,17]])-1, # indices of gram coefficnents corresponding to bones lenght, must be changed depending on the skeleton used\n",
    "                 Bone_Loss_coeff = 100,\n",
    "                 ):\n",
    "\n",
    "        self.session = session\n",
    "        self.size_SRVF_H = size_SRVF_H\n",
    "        self.size_SRVF_W = size_SRVF_W\n",
    "        self.nb_frames = nb_frames\n",
    "        self.size_kernel = size_kernel\n",
    "        self.Gram_matrix_size = Gram_matrix_size\n",
    "        self.size_batch = size_batch\n",
    "        self.num_input_channels = num_input_channels\n",
    "        self.num_encoder_channels = num_encoder_channels\n",
    "        self.num_z_channels = num_z_channels\n",
    "        self.y_dim = y_dim\n",
    "        self.rb_dim = rb_dim\n",
    "        self.test_batch_low=test_batch_low\n",
    "        self.test_batch_up=test_batch_up\n",
    "        self.num_gen_channels = num_gen_channels\n",
    "        self.enable_tile_label = enable_tile_label\n",
    "        self.tile_ratio = tile_ratio\n",
    "        self.is_training = is_training\n",
    "        self.data_dir = data_dir\n",
    "        self.load_train =load_train\n",
    "        self.load_test = load_test\n",
    "        self.load_qmean=load_qmean\n",
    "        self.save_dir = save_dir\n",
    "        self.is_flip = is_flip\n",
    "        self.checkpoint_dir = checkpoint_dir + discription\n",
    "        self.checkpoint_name = checkpoint_name\n",
    "        self.generated_dir=generated_dir\n",
    "        self.disc_iters = disc_iters\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.LAMBDA = LAMBDA\n",
    "        self.discription = discription\n",
    "        self.coeff_skel_loss = coeff_skel_loss\n",
    "        self.Skel_links = Skel_links\n",
    "        self.Bone_Loss_coeff = Bone_Loss_coeff\n",
    "        print(self.discription)\n",
    "\n",
    "        ###reference point\n",
    "        Q_ref_ = loadmat(self.load_qmean)\n",
    "        q_mean = Q_ref_['q_mean']\n",
    "        self.Q_ref = np.zeros([self.size_batch, self.size_SRVF_H, self.size_SRVF_W])\n",
    "        for i in range(self.Q_ref.shape[0]):\n",
    "            self.Q_ref[i, :, :] = q_mean\n",
    "\n",
    "        self.Q_ref_tensor = tf.constant(self.Q_ref, dtype=tf.float32)\n",
    "\n",
    "        self.indices = []\n",
    "        self.lenlinks = len(Skel_links)\n",
    "        for b in range(0, self.size_batch):\n",
    "            for f in range(0, self.size_SRVF_W):\n",
    "                for j in range(0, self.lenlinks):\n",
    "                    self.indices.append([b, f, Skel_links[j, 0], Skel_links[j, 1]])\n",
    "\n",
    "        self.indices_L = []\n",
    "        self.indices_R = []\n",
    "        for b in range(0, self.size_batch):\n",
    "            for f in range(0, self.size_SRVF_W):\n",
    "                for d in range(0, 3):\n",
    "                    for j in range(0, self.lenlinks):\n",
    "                        self.indices_L.append([b, f, d, self.Skel_links[j, 0]])\n",
    "                        self.indices_R.append([b, f, d, self.Skel_links[j, 1]])\n",
    "\n",
    "        print(\"\\n\\tLoading data\")\n",
    "        self.data_y, self.data_X, self.first_frames, self.Joints = self.load_data(self.load_train)\n",
    "        self.data_X = [os.path.join(self.data_dir, x) for x in self.data_X]\n",
    "        # get path for y SRVF file\n",
    "        self.data_y = [os.path.join(self.data_dir, y) for y in self.data_y]\n",
    "        self.first_frames = [os.path.join(self.data_dir, f) for f in self.first_frames]\n",
    "        self.Joints = [os.path.join(self.data_dir, j) for j in self.Joints]\n",
    "\n",
    "\n",
    "        self.real_data = tf.compat.v1.placeholder(\n",
    "            tf.float32,\n",
    "            [self.size_batch, self.size_SRVF_H * self.size_SRVF_W],\n",
    "            name='real_data'\n",
    "        )\n",
    "        self.emotion = tf.compat.v1.placeholder(\n",
    "            tf.float32,\n",
    "            #[self.size_batch, self.y_dim * self.rb_dim],\n",
    "            [self.size_batch, self.size_SRVF_H * self.nb_frames], # changement de dimensions\n",
    "            name='emotion_labels'\n",
    "        )\n",
    "\n",
    "        self.first_frames_real = tf.compat.v1.placeholder(\n",
    "            tf.float32,\n",
    "            [self.size_batch ,self.size_SRVF_H, self.size_SRVF_W],\n",
    "            name='first_skeleton_frames'\n",
    "        )\n",
    "\n",
    "        self.Joints_pos_real = tf.compat.v1.placeholder(\n",
    "            tf.float32,\n",
    "            [self.size_batch, self.size_SRVF_H, self.size_SRVF_W],\n",
    "            name='joint_position'\n",
    "        )\n",
    "\n",
    "        self.log_real = self.log_map(self.real_data)\n",
    "        #self.fake_data, self.Lands_Gen = self.Generator(self.emotion, self.first_frames_real)\n",
    "        self.fake_data = self.Generator(self.emotion, self.first_frames_real)\n",
    "        self.exp_fake = self.exp_map(self.fake_data)\n",
    "        self.log_exp_fake = self.log_map(self.exp_fake)\n",
    "        self.disc_log_real = self.Discriminator(self.log_real, self.emotion, enable_bn=True)\n",
    "        self.disc_log_exp_fake = self.Discriminator(self.log_exp_fake, self.emotion, reuse_variables=True,\n",
    "                                                    enable_bn=True)\n",
    "\n",
    "        ############### losses to minimize \n",
    "        ## reconstruction_loss = tf.nn.l2.loss(log_exp_fake - log_real)   #L2 loss\n",
    "        reconstruction_loss = tf.reduce_mean(tf.abs(self.log_real - self.log_exp_fake))  # L1 loss\n",
    "        Gram_loss, Bone_loss =  self.Gram_loss_func(self.exp_fake, self.Joints_pos_real,self.first_frames_real)\n",
    "        self.gen_cost = -tf.reduce_mean(self.disc_log_exp_fake) + reconstruction_loss + self.coeff_skel_loss *Gram_loss + self.Bone_Loss_coeff * Bone_loss\n",
    "        self.gen_cost_ = -self.gen_cost\n",
    "        self.help_loss = tf.reduce_mean(self.disc_log_real)\n",
    "        self.disc_cost = tf.reduce_mean(self.disc_log_exp_fake) - tf.reduce_mean(\n",
    "            self.disc_log_real)  ###+ reconstruction_loss\n",
    "\n",
    "        # penalty of improved WGAN\n",
    "        alpha = tf.random.uniform(\n",
    "            shape=[self.size_batch, 1],\n",
    "            minval=0.,\n",
    "            maxval=1.\n",
    "        )\n",
    "        differences = self.log_exp_fake - self.log_real\n",
    "        interpolates = self.log_real + (alpha * differences)\n",
    "        gradients = tf.gradients(self.Discriminator(interpolates, self.emotion, reuse_variables=True, enable_bn=True),\n",
    "                                 [interpolates])[0]\n",
    "        slopes = tf.sqrt(tf.compat.v1.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "        self.disc_cost += self.LAMBDA * gradient_penalty\n",
    "\n",
    "        trainable_variables = tf.compat.v1.trainable_variables()  ##returns all variables created(the two variable scopes) and makes trainable true\n",
    "        self.gen_params = [var for var in trainable_variables if 'G_' in var.name]\n",
    "        self.disc_params = [var for var in trainable_variables if 'D_' in var.name]\n",
    "\n",
    "        GEN_cost_summary = tf.compat.v1.summary.scalar('GEN_cost', self.gen_cost_)\n",
    "        DISC_cost_summary = tf.compat.v1.summary.scalar('DISC_cost', self.disc_cost)\n",
    "\n",
    "        GENerator_cost_summary=tf.compat.v1.summary.scalar('GENerator_cost', -tf.reduce_mean(self.disc_log_exp_fake))\n",
    "        reconstruction_cost_summary = tf.compat.v1.summary.scalar('Reconstruction_cost', reconstruction_loss)\n",
    "        Gram_cost_summary =  tf.compat.v1.summary.scalar('Gram_cost', self.coeff_skel_loss * Gram_loss)\n",
    "        Bone_cost_summary = tf.compat.v1.summary.scalar('Bone_cost', self.Bone_Loss_coeff * Bone_loss)\n",
    "\n",
    "\n",
    "        help_cost_summary = tf.compat.v1.summary.scalar('DiscReal_cost', self.help_loss)\n",
    "        self.summary = tf.compat.v1.summary.merge(\n",
    "            [GEN_cost_summary, DISC_cost_summary, GENerator_cost_summary,reconstruction_cost_summary,Gram_cost_summary, Bone_cost_summary, help_cost_summary])\n",
    "\n",
    "        self.saver = tf.compat.v1.train.Saver(max_to_keep=50, keep_checkpoint_every_n_hours=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bba47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def load_data(self, file):\n",
    "            # get path to data file x = input, y= condition , frame = first frame of reaction sequence, gram = path to folder with gram matrix for each frame\n",
    "            X = []\n",
    "            y = []\n",
    "            fframe = []\n",
    "            frames =[]\n",
    "            for line in open(file, 'r'):\n",
    "                data = line.split()\n",
    "                X.append(data[0])\n",
    "                y.append(data[1])\n",
    "                fframe.append(data[2])\n",
    "                frames.append(data[3])\n",
    "            seed = 2019\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(X)\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(y)\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(fframe)\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(frames)\n",
    "            return X, y, fframe,frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99f848c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def load_data_test(self, file):\n",
    "            # get path to data file x = input, y= condition , frame = first frame of reaction sequence, gram = path to folder with gram matrix for each frame\n",
    "            X = []\n",
    "            y = []\n",
    "            fframe = []\n",
    "            for line in open(file, 'r'):\n",
    "                data = line.split()\n",
    "                X.append(data[0])\n",
    "                y.append(data[1])\n",
    "                fframe.append(data[2])\n",
    "            seed = 2019\n",
    "            return X, y, fframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee709f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def read_SRVF(self, path_SRVF):\n",
    "            data_ = loadmat(path_SRVF)\n",
    "            data = data_['q2n']\n",
    "            data = np.reshape(data, [data.shape[0] * data.shape[1]])\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4140fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def read_first_frame(self, path_frame):\n",
    "            data_ = np.loadtxt(path_frame, delimiter=',')\n",
    "            a = data_[:, 0]\n",
    "            b = data_[:, 1]\n",
    "            c = data_[:, 2]\n",
    "            d = np.hstack([a, b])\n",
    "            e = np.hstack([d, c])\n",
    "            data = e\n",
    "            for f in range(1, self.size_SRVF_W):\n",
    "                data = np.vstack((data, e))\n",
    "            data = np.transpose(data)\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a947a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def read_frames(self, path):\n",
    "            data_ = loadmat(path)\n",
    "            if \"_A\" in path:\n",
    "                data = data_['curve_A']\n",
    "            else:\n",
    "                data = data_['curve_B']\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72cede33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def Inner(self, A, B):\n",
    "            [m, n, T] = A.get_shape().as_list()\n",
    "            A_B = [A, B]\n",
    "            mult = tf.map_fn(lambda a_b: a_b[0] * a_b[1], A_B, dtype=tf.float32)  ##A*A  ##tf.multiply(A,A)\n",
    "            s1 = tf.reduce_sum(mult, 1, keepdims=False)\n",
    "            s2 = tf.reduce_sum(s1, 1, keepdims=False) / T\n",
    "            ##norm=tf.sqrt(s2)\n",
    "            return s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00bdf8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def q_to_curves(self, q):\n",
    "\n",
    "        s = tf.linspace(0.0, 1.0, self.size_SRVF_W)\n",
    "        qnorme = tf.norm(q, ord=2, axis=[1, 3], keepdims=None, name=None)\n",
    "        qnorm = tf.expand_dims(qnorme, 1)\n",
    "\n",
    "        qnorm = tf.squeeze(qnorm)\n",
    "        QN = tf.repeat(qnorm, repeats=self.size_SRVF_H, axis=0)\n",
    "        QN = tf.reshape(QN, [self.size_batch, self.size_SRVF_H, self.size_SRVF_W])\n",
    "        temp = tf.math.multiply(tf.squeeze(q), QN)\n",
    "        curve = self.cumultrapz(temp, s)\n",
    "\n",
    "        return curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cff09065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def Gram_loss_func(self, Landmarks, Landmarks_real,first_frame):\n",
    "\n",
    "            Landmarks = tf.reshape(Landmarks,[self.size_batch, self.size_SRVF_H, self.size_SRVF_W,1])\n",
    "            if self.is_training:\n",
    "                curves = self.q_to_curves(Landmarks)\n",
    "                Lands = curves + first_frame\n",
    "            else:\n",
    "                Lands = tf.Variable(tf.zeros([self.size_batch, self.size_SRVF_H, self.size_SRVF_W], tf.float32))\n",
    "\n",
    "            n_dim = 3\n",
    "            Joints_real = tf.reshape(Landmarks_real, [self.size_batch, n_dim, int(self.size_SRVF_H / n_dim), self.size_SRVF_W])\n",
    "            Joints_fake = tf.reshape(Lands,  [self.size_batch, n_dim, int(self.size_SRVF_H / n_dim), self.size_SRVF_W])\n",
    "            G = tf.linalg.matmul(tf.transpose(Joints_fake, perm=[0, 3, 1, 2]), tf.transpose(Joints_real, perm=[0, 3, 2, 1]))\n",
    "            sig, u, v = tf.linalg.svd(G)\n",
    "            ssig = tf.reduce_sum(sig, axis=[2])\n",
    "            Gram_Real = tf.linalg.matmul(tf.transpose(Joints_real, perm=[0, 3, 2, 1]), tf.transpose(Joints_real, perm=[0, 3, 1, 2]))\n",
    "            Gram_Fake = tf.linalg.matmul(tf.transpose(Joints_fake, perm=[0, 3, 2, 1]), tf.transpose(Joints_fake, perm=[0, 3, 1, 2]))\n",
    "            L = tf.linalg.trace(Gram_Real) + tf.linalg.trace(Gram_Fake) - 2.0 * ssig\n",
    "            Gram_Loss = tf.reduce_mean(L)\n",
    "\n",
    "            #Bones_Real = tf.gather_nd(Gram_Real, self.indices)\n",
    "            #Bones_Real = tf.reshape(Bones_Real, [self.size_batch, self.size_SRVF_W, self.lenlinks])\n",
    "            #Bones_Fake = tf.gather_nd(Gram_Fake, self.indices)\n",
    "            #Bones_Fake = tf.reshape(Bones_Fake, [self.size_batch, self.size_SRVF_W, self.lenlinks])\n",
    "            #diff= Bones_Real-Bones_Fake\n",
    "            #norme = tf.norm(diff, ord='euclidean', axis=2)\n",
    "            #Bone_Loss = tf.reduce_mean(norme)\n",
    "\n",
    "            Joints_fake = tf.transpose(Joints_fake, perm=[0, 3, 1, 2])\n",
    "            Joints_real = tf.transpose(Joints_real, perm=[0, 3, 1, 2])\n",
    "            J1 = tf.gather_nd(Joints_real, self.indices_L)\n",
    "            J1 = tf.reshape(J1, [self.size_batch, self.size_SRVF_W, 3, self.lenlinks])\n",
    "            J2 = tf.gather_nd(Joints_real, self.indices_R)\n",
    "            J2 = tf.reshape(J2, [self.size_batch, self.size_SRVF_W, 3, self.lenlinks])\n",
    "            B1 = J1 - J2\n",
    "            B1 = tf.norm(B1, ord='euclidean', axis=2)\n",
    "            J3 = tf.gather_nd(Joints_fake, self.indices_L)\n",
    "            J3 = tf.reshape(J3, [self.size_batch, self.size_SRVF_W, 3, self.lenlinks])\n",
    "            J4 = tf.gather_nd(Joints_fake, self.indices_R)\n",
    "            J4 = tf.reshape(J4, [self.size_batch, self.size_SRVF_W, 3, self.lenlinks])\n",
    "            B2 = J3 - J4\n",
    "            B2 = tf.norm(B2, ord='euclidean', axis=2)\n",
    "            diff3 = B1 - B2\n",
    "            norme_3 = tf.norm(diff3, ord='euclidean', axis=2)\n",
    "            Bone_Loss = tf.reduce_mean(norme_3)\n",
    "\n",
    "            return Gram_Loss,Bone_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee7c0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def cumultrapz(self,y,x):\n",
    "            dx = (x[1] - x[0])\n",
    "            Y1 = y[:, :, 0:-1]\n",
    "            Y2 = y[:, :, 1:]\n",
    "            inte = ((Y1 + Y2) / 2.0) * dx\n",
    "            zer = tf.Variable(tf.zeros([self.size_batch, self.size_SRVF_H, self.size_SRVF_W - 1], tf.float32))\n",
    "            integ = tf.concat([zer, inte], axis=2)\n",
    "            cumul = tf.math.cumsum(integ[:, :, self.size_SRVF_W - 2:], axis=2)\n",
    "            return cumul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a4ecf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def exp_map(self, q):\n",
    "            q = tf.reshape(q, [self.size_batch, self.size_SRVF_H, self.size_SRVF_W])\n",
    "            [m, n, T] = q.get_shape().as_list()\n",
    "            lw = tf.sqrt(self.Inner(q, q))\n",
    "            res = self.Q_ref_tensor * tf.expand_dims(tf.expand_dims(tf.cos(lw), -1), -1) + q * (\n",
    "                tf.expand_dims(tf.expand_dims(tf.sin(lw) / lw, -1), -1))\n",
    "            return tf.reshape(res, [self.size_batch, self.size_SRVF_H * self.size_SRVF_W])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5023c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def log_map(self, q):\n",
    "            q = tf.reshape(q, [self.size_batch, self.size_SRVF_H, self.size_SRVF_W])\n",
    "            [m, n, T] = q.get_shape().as_list()\n",
    "            prod = self.Inner(self.Q_ref_tensor, q)\n",
    "            u = q - self.Q_ref_tensor * tf.expand_dims(tf.expand_dims(prod, -1), -1)\n",
    "            u = tf.cast(u, tf.float32)\n",
    "            lu = tf.sqrt(self.Inner(u, u))\n",
    "            theta = tf.acos(tf.clip_by_value(prod, -0.98, 0.98))  ###tf.acos(prod)\n",
    "            zero = tf.constant(0, shape=[m], dtype=tf.float32)\n",
    "\n",
    "            def f1(): return tf.cast(u * tf.expand_dims(tf.expand_dims(zero, -1), -1), tf.float32)\n",
    "\n",
    "            def f2(): return tf.cast(u * tf.expand_dims(tf.expand_dims(theta / lu, -1), -1), tf.float32)\n",
    "\n",
    "            res = tf.cond(tf.reduce_all(tf.equal(lu, zero)), f1, f2)\n",
    "            return tf.reshape(res, [self.size_batch, self.size_SRVF_H * self.size_SRVF_W])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71b28d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def geodesic_dist(self, q1, q2):\n",
    "            q1 = tf.reshape(q1, [self.size_batch, self.size_SRVF_H, self.size_SRVF_W])\n",
    "            q2 = tf.reshape(q2, [self.size_batch, self.size_SRVF_H, self.size_SRVF_W])\n",
    "            inner_prod = self.Inner(q1, q2)\n",
    "            dist = tf.acos(tf.clip_by_value(inner_prod, -0.98, 0.98))  ## ds=acos(<q1,q2>)\n",
    "            return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c642d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def save_checkpoint(self):\n",
    "            checkpoint_dir = os.path.join(self.save_dir, self.checkpoint_dir)\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            self.saver.save(\n",
    "                sess=self.session,\n",
    "                save_path=os.path.join(checkpoint_dir, 'model'),\n",
    "                global_step=self.global_step.eval()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d020148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def load_checkpoint(self, dir):\n",
    "            print(\"\\n\\tLoading pre-trained model ...\")\n",
    "            checkpoint_dir = dir\n",
    "            print(checkpoint_dir)\n",
    "            checkpoints = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "            if checkpoints and checkpoints.model_checkpoint_path:\n",
    "                checkpoints_name = self.checkpoint_name ##os.path.basename(checkpoints.model_checkpoint_path)\n",
    "                self.saver.restore(self.session, os.path.join(checkpoint_dir, checkpoints_name))\n",
    "                return True\n",
    "            else:\n",
    "                return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83674198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def y_to_rb_label(self, label):\n",
    "            # inutilise\n",
    "            number = np.argmax(label)\n",
    "            one_hot = np.random.uniform(-1, 1, self.rb_dim)\n",
    "            rb = np.tile(-1 * np.abs(one_hot), self.y_dim)\n",
    "            rb[number * self.rb_dim:(number + 1) * self.rb_dim] = np.abs(one_hot)\n",
    "            return rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f21f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def Generator(self, y, first_frame, noise=None, reuse_variables=False, enable_tile_label=True, tile_ratio=1.0):\n",
    "            if reuse_variables:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            if noise is None:\n",
    "                noise = tf.random.normal([self.size_batch, 1])\n",
    "                #fframe_to_concat = first_frame[:, :, 1]\n",
    "                noise = tf.concat([noise, y], 1)\n",
    "                #noise = y\n",
    "            if (self.size_SRVF_H > self.size_SRVF_W):\n",
    "                num_layers = int(np.log2(self.size_SRVF_W)) - int(self.size_kernel / 2)\n",
    "            else:\n",
    "                num_layers = int(np.log2(self.size_SRVF_H)) - int(self.size_kernel / 2)\n",
    "            ## TODO: try concat_label used in ExprGAN. In this case, 6 channels will be added to the output without changing the size of feature maps\n",
    "            duplicate = 1\n",
    "            #z = concat_label_newtf(noise, y, duplicate=duplicate)\n",
    "            z=noise\n",
    "            size_mini_map_H = int(self.size_SRVF_H / 2 ** num_layers)\n",
    "            size_mini_map_W = int(self.size_SRVF_W / 2 ** num_layers)\n",
    "            name = 'G_fc'\n",
    "            current = fc(\n",
    "                input_vector=z,\n",
    "                num_output_length=self.num_gen_channels * size_mini_map_H * size_mini_map_W,\n",
    "                name=name\n",
    "            )\n",
    "            current = tf.reshape(current, [-1, size_mini_map_H, size_mini_map_W, self.num_gen_channels])\n",
    "            current = tf.nn.relu(current)\n",
    "            #current = concat_label_newtf(current, y) # remove intermediate concat\n",
    "            for i in range(num_layers):\n",
    "                name = 'G_deconv' + str(i)\n",
    "                current = tf.compat.v1.image.resize_nearest_neighbor(current, [size_mini_map_H * 2 ** (i + 1),\n",
    "                                                                               size_mini_map_W * 2 ** (i + 1)])\n",
    "                current = custom_conv2d(input_map=current, num_output_channels=int(self.num_gen_channels / 2 ** (i + 1)),\n",
    "                                        name=name)\n",
    "                current = tf.nn.relu(current)\n",
    "                #current = concat_label_newtf(current, y) # remove intermediate concat\n",
    "            name = 'G_deconv' + str(i + 1)\n",
    "            current = tf.compat.v1.image.resize_nearest_neighbor(current, [self.size_SRVF_H, self.size_SRVF_W])\n",
    "            current = custom_conv2d(input_map=current, num_output_channels=int(self.num_gen_channels / 2 ** (i + 2)),\n",
    "                                    name=name)\n",
    "            current = tf.nn.relu(current)\n",
    "\n",
    "            #current = concat_label_newtf(current, y) #remove intermediate concat\n",
    "            name = 'G_deconv' + str(i + 2)\n",
    "            current = custom_conv2d(input_map=current, num_output_channels=self.num_input_channels,\n",
    "                                    name=name)  ### output format: NHWC\n",
    "            generated_image_ = tf.nn.tanh(current)\n",
    "            generated_image = tf.reshape(generated_image_, [self.size_batch, self.size_SRVF_H * self.size_SRVF_W])\n",
    "            return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "001ca6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def Discriminator(self, z, y, is_training=True, reuse_variables=False, num_hidden_layer_channels=(64, 32, 16),\n",
    "                          enable_bn=True):\n",
    "            if reuse_variables:\n",
    "                tf.compat.v1.get_variable_scope().reuse_variables()\n",
    "            num_layers = len(num_hidden_layer_channels)\n",
    "            current = tf.reshape(z, [self.size_batch, self.size_SRVF_H, self.size_SRVF_W, 1])  ##generated_image\n",
    "            current = concat_label_newtf(current, y)\n",
    "            for i in range(num_layers):\n",
    "                print(i)\n",
    "                name = 'D_img_conv' + str(i)\n",
    "                current = conv2d(\n",
    "                    input_map=current,\n",
    "                    num_output_channels=num_hidden_layer_channels[i],\n",
    "                    size_kernel=self.size_kernel,\n",
    "                    name=name\n",
    "                )\n",
    "                print(current.get_shape())\n",
    "                if enable_bn:\n",
    "                    name = 'D_img_bn' + str(i)\n",
    "                    current = tf.compat.v1.layers.batch_normalization(\n",
    "                        current,\n",
    "                        scale=False,\n",
    "                        training=is_training,\n",
    "                        name=name,\n",
    "                        reuse=reuse_variables\n",
    "                    )\n",
    "                current = tf.nn.relu(current)\n",
    "                #current = concat_label_newtf(current, y) #remove intermediate concat\n",
    "                print(current.get_shape())\n",
    "            name = 'D_img_fc1'\n",
    "            current = fc(\n",
    "                input_vector=tf.reshape(current, [self.size_batch, -1]),\n",
    "                num_output_length=1024,\n",
    "                name=name\n",
    "            )\n",
    "            name = 'D_img_fc1_bn'\n",
    "            current = tf.compat.v1.layers.batch_normalization(\n",
    "                current,\n",
    "                scale=False,\n",
    "                training=is_training,\n",
    "                name=name,\n",
    "                reuse=reuse_variables\n",
    "            )\n",
    "            current = lrelu(current)\n",
    "            #current = concat_label_newtf(current, y) #remove intermediate concat\n",
    "            name = 'D_img_fc2'\n",
    "            disc = fc(\n",
    "                input_vector=current,\n",
    "                num_output_length=1,\n",
    "                name=name\n",
    "            )\n",
    "            return disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18a9f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def test(self, random_seed, dir):\n",
    "            if not self.load_checkpoint(dir):\n",
    "                print(\"\\tFAILED >_<!\")\n",
    "                exit(0)\n",
    "            else:\n",
    "                print(\"\\tSUCCESS ^_^\")\n",
    "            data_xtest, data_ytest, first_frame = self.load_data_test(self.load_test)\n",
    "            data_xtest = [os.path.join(self.data_dir, x) for x in data_xtest]\n",
    "            data_ytest = [os.path.join(self.data_dir, y) for y in data_ytest]\n",
    "            first_frame = [os.path.join(self.data_dir, f) for f in first_frame]\n",
    "\n",
    "\n",
    "            batch = [self.read_SRVF(path_SRVF=batch_file) for batch_file in data_xtest]\n",
    "            batch_ff = [self.read_first_frame(path_frame=batch_file) for batch_file in first_frame]\n",
    "\n",
    "            batch_label_rb = np.array(batch).astype(np.float32)\n",
    "            batch_first_frame = np.array(batch_ff).astype(np.float32)\n",
    "\n",
    "            SRVF_generated = tf.reshape(self.exp_fake, [len(data_xtest), self.size_SRVF_H, self.size_SRVF_W])\n",
    "            norm = tf.sqrt(self.Inner(SRVF_generated, SRVF_generated))\n",
    "            SRVF_generat = self.session.run(SRVF_generated, feed_dict={self.emotion: batch_label_rb,\n",
    "                                                                       self.first_frames_real: batch_first_frame})\n",
    "\n",
    "            res = self.session.run(norm,\n",
    "                                   feed_dict={self.emotion: batch_label_rb, self.first_frames_real: batch_first_frame})\n",
    "            #mean_res = sum(res) /len(data_xtest)\n",
    "            k = 0\n",
    "            for i in range(0,len(res)):\n",
    "                if 0.9 < res[i] < 1.1:\n",
    "                    k = k + 1\n",
    "\n",
    "            batch_labels = [self.read_SRVF(path_SRVF=batch_file) for batch_file in data_ytest]\n",
    "            for i in range(0,len(data_xtest)):\n",
    "                nb = i + 1\n",
    "                save_path = self.generated_dir + '/test_' + str(nb) + '.mat'\n",
    "                scipy.io.savemat(save_path, dict([('x_test', SRVF_generat[i])]))\n",
    "\n",
    "            print('Samples generated. Saved to ' +save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b17d0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRVF_WGAN(SRVF_WGAN):\n",
    "    def train(self,\n",
    "                  num_epochs=200,\n",
    "                  decay_rate=1.0,\n",
    "                  enable_shuffle=True,\n",
    "                  ):\n",
    "\n",
    "            ## count number of batches seen by the graph\n",
    "            self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            Train_learning_rate = tf.compat.v1.train.exponential_decay(\n",
    "                learning_rate=self.learning_rate,\n",
    "                global_step=self.global_step,\n",
    "                decay_steps=1000,  ##len(self.data_X) // self.size_batch * 2,\n",
    "                decay_rate=decay_rate,\n",
    "                staircase=True\n",
    "            )\n",
    "            with tf.compat.v1.variable_scope('gen-optimize',\n",
    "                                             reuse=tf.compat.v1.AUTO_REUSE):  ##tf.get_variable_scope(),reuse=tf.AUTO_REUSE):\n",
    "                self.gen_train_op = tf.compat.v1.train.AdamOptimizer(\n",
    "                    learning_rate=Train_learning_rate,\n",
    "                    beta1=0.5,\n",
    "                    beta2=0.9\n",
    "                ).minimize(self.gen_cost, global_step=self.global_step, var_list=self.gen_params)\n",
    "            with tf.compat.v1.variable_scope('disc-optimizer', reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                self.disc_train_op = tf.compat.v1.train.AdamOptimizer(\n",
    "                    learning_rate=Train_learning_rate,\n",
    "                    beta1=0.5,\n",
    "                    beta2=0.9\n",
    "                ).minimize(self.disc_cost, global_step=self.global_step, var_list=self.disc_params)\n",
    "\n",
    "            ## write summary          \n",
    "            filename = 'summary' + str(self.learning_rate) + self.discription\n",
    "            self.writer = tf.compat.v1.summary.FileWriter(os.path.join(self.save_dir, filename),\n",
    "                                                          self.session.graph)  ##train.SummaryWriter\n",
    "            try:\n",
    "                tf.global_variables_initializer().run()\n",
    "            except:\n",
    "                tf.compat.v1.initialize_all_variables().run()\n",
    "            num_batches = len(self.data_X) // self.size_batch\n",
    "            for epoch in range(num_epochs):\n",
    "                if enable_shuffle:\n",
    "                    seed = 2019\n",
    "                    np.random.seed(seed)\n",
    "                    np.random.shuffle(self.data_X)\n",
    "                    np.random.seed(seed)\n",
    "                    np.random.shuffle(self.data_y)\n",
    "                    np.random.seed(seed)\n",
    "                    np.random.shuffle(self.first_frames)\n",
    "                    np.random.seed(seed)\n",
    "                    np.random.shuffle(self.Joints)\n",
    "                for ind_batch in range(num_batches):\n",
    "                    start_time = time.time()\n",
    "                    batch_files = self.data_X[ind_batch * self.size_batch:(ind_batch + 1) * self.size_batch]\n",
    "                    batch = [self.read_SRVF(\n",
    "                        path_SRVF=batch_file) for batch_file in batch_files]\n",
    "                    batch_SRVF = np.array(batch).astype(np.float32)\n",
    "\n",
    "                    ## utilise les SRVF au lieu des matrices de labels\n",
    "                    batch_files_emo = self.data_y[ind_batch * self.size_batch:(ind_batch + 1) * self.size_batch]\n",
    "                    batch_emo = [self.read_SRVF(\n",
    "                        path_SRVF=batch_file) for batch_file in batch_files_emo]\n",
    "                    batch_label_emo = np.array(batch_emo).astype(np.float32)\n",
    "\n",
    "                    batch_files_frame = self.first_frames[ind_batch * self.size_batch:(ind_batch + 1) * self.size_batch]\n",
    "                    batch_fframe = [self.read_first_frame(\n",
    "                        path_frame=batch_file) for batch_file in batch_files_frame]\n",
    "                    batch_first_frame = np.array(batch_fframe).astype(np.float32)\n",
    "\n",
    "                    batch_files_frames = self.Joints[ind_batch * self.size_batch:(ind_batch + 1) * self.size_batch]\n",
    "                    batch_frames = [self.read_frames(\n",
    "                        path=batch_file) for batch_file in batch_files_frames]\n",
    "                    batch_all_frames = np.array(batch_frames).astype(np.float32)\n",
    "\n",
    "                    G_err, _ = self.session.run([self.gen_cost_, self.gen_train_op],\n",
    "                                                feed_dict={self.real_data: batch_SRVF, self.emotion: batch_label_emo,\n",
    "                                                           self.first_frames_real: batch_first_frame, self.Joints_pos_real: batch_all_frames})\n",
    "                    for I in range(self.disc_iters):\n",
    "                        D_err, _ = self.session.run([self.disc_cost, self.disc_train_op],\n",
    "                                                    feed_dict={self.real_data: batch_SRVF, self.emotion: batch_label_emo, self.first_frames_real: batch_first_frame})\n",
    "\n",
    "                    print(\"\\nEpoch: [%3d/%3d] Batch: [%3d/%3d]\\n\\tD_err=%.4f \\n\\tG_err=%.4f\" %\n",
    "                          (epoch + 1, num_epochs, ind_batch + 1, num_batches, D_err, G_err))\n",
    "                    elapse = time.time() - start_time\n",
    "                    time_left = ((self.num_epochs - epoch - 1) * num_batches + (num_batches - ind_batch - 1)) * elapse\n",
    "                    print(\"\\tTime left: %02d:%02d:%02d\" %\n",
    "                          (int(time_left / 3600), int(time_left % 3600 / 60), time_left % 60))\n",
    "                    summary = self.summary.eval(\n",
    "                        feed_dict={\n",
    "                            self.real_data: batch_SRVF,\n",
    "                            self.emotion: batch_label_emo,\n",
    "                            self.first_frames_real: batch_first_frame,\n",
    "                            self.Joints_pos_real: batch_all_frames\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    self.writer.add_summary(summary, self.global_step.eval())\n",
    "                    if np.mod(epoch+1, 100) == 0:\n",
    "                       self.save_checkpoint()\n",
    "            self.save_checkpoint()\n",
    "            self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2511785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN\n",
      "\n",
      "\tLoading data\n",
      "WARNING:tensorflow:From C:\\Users\\baptiste chopin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\baptiste chopin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(5, 5, 1276, 64)\n",
      "(644, 26, 13, 64)\n",
      "WARNING:tensorflow:From <ipython-input-27-067f4afc2c5f>:26: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-067f4afc2c5f>:26: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\baptiste chopin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\layers\\normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\baptiste chopin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\layers\\normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(644, 26, 13, 64)\n",
      "1\n",
      "(5, 5, 64, 32)\n",
      "(644, 13, 7, 32)\n",
      "(644, 13, 7, 32)\n",
      "2\n",
      "(5, 5, 32, 16)\n",
      "(644, 7, 4, 16)\n",
      "(644, 7, 4, 16)\n",
      "0\n",
      "(5, 5, 1276, 64)\n",
      "(644, 26, 13, 64)\n",
      "(644, 26, 13, 64)\n",
      "1\n",
      "(5, 5, 64, 32)\n",
      "(644, 13, 7, 32)\n",
      "(644, 13, 7, 32)\n",
      "2\n",
      "(5, 5, 32, 16)\n",
      "(644, 7, 4, 16)\n",
      "(644, 7, 4, 16)\n",
      "0\n",
      "(5, 5, 1276, 64)\n",
      "(644, 26, 13, 64)\n",
      "(644, 26, 13, 64)\n",
      "1\n",
      "(5, 5, 64, 32)\n",
      "(644, 13, 7, 32)\n",
      "(644, 13, 7, 32)\n",
      "2\n",
      "(5, 5, 32, 16)\n",
      "(644, 7, 4, 16)\n",
      "(644, 7, 4, 16)\n",
      "\n",
      "\tTesting Mode\n",
      "\n",
      "\tLoading pre-trained model ...\n",
      "Checkpoint_long\n",
      "INFO:tensorflow:Restoring parameters from Checkpoint_long\\model-107500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Checkpoint_long\\model-107500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSUCCESS ^_^\n",
      "Samples generated. Saved to C:\\Users\\baptiste chopin\\PycharmProjects\\Prediction_jupyter\\generated_samples_long/test_644.mat\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session(config=config) as session:\n",
    "    model = SRVF_WGAN(\n",
    "        session,\n",
    "        is_training=FLAGS.is_train,\n",
    "        save_dir=FLAGS.save_dir,\n",
    "        checkpoint_dir=FLAGS.checkpoint_dir,\n",
    "        size_batch=FLAGS.batch_size,\n",
    "        load_train=FLAGS.load_train,\n",
    "        load_test = FLAGS.load_test,\n",
    "        size_SRVF_W = FLAGS.number_frames,\n",
    "        nb_frames = FLAGS.number_frames_prior,\n",
    "        size_SRVF_H = FLAGS.nb_joints*3,\n",
    "        coeff_skel_loss=FLAGS.Coeff_Gram,\n",
    "        Bone_Loss_coeff =FLAGS.Coeff_bone,\n",
    "        load_qmean = FLAGS.load_qmean,\n",
    "        checkpoint_name=FLAGS.checkpoint_name,\n",
    "        generated_dir = FLAGS.generated_dir,\n",
    "        data_dir = FLAGS.data_dir\n",
    "\n",
    "    )\n",
    "    if FLAGS.is_train:\n",
    "        print('\\n\\tTraining Mode')\n",
    "        model.train(\n",
    "            num_epochs=FLAGS.epoch  # reduit pour rapidite\n",
    "        )\n",
    "    else:\n",
    "        seed = 2019\n",
    "        print('\\n\\tTesting Mode')\n",
    "        model.test(random_seed=seed, dir=FLAGS.checkpoint_dir,\n",
    "                   # label 1,2,3 pour premier SRVF, label 4,5,6 pour deuxieme SRVF\n",
    "                   )\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
